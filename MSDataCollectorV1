import os
import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin

# --- DESTINATION ---
SAVE_DIR = r"C:\Users\David\Documents\School Stuff\UW Things\Classwork\ESS 469\MLGEO_Hydrothermal_Vents\masspa_2017_datadump"
BASE_URL = "https://rawdata.oceanobservatories.org/files/RS01SUM2/MJ01B/MASSPA101/"

if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

# Track where we've been so we don't loop
visited_urls = set()

HEADERS = {'User-Agent': 'Mozilla/5.0'}

def download_txt_recursive(url):
    # If we've already scrubbed this folder, go back up
    if url in visited_urls:
        return
    visited_urls.add(url)

    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        if response.status_code != 200: return 

        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all('a')

        for link in links:
            href = link.get('href')
            
            # 1. IGNORE the "Parent Directory" and "Self" links
            # This is key! We let Python handle the "going back up," NOT the URL.
            if not href or href.startswith('?') or href in ['/', '../', './']:
                continue 
            
            full_url = urljoin(url, href)

            # --- FOLDER LOGIC ---
            if href.endswith('/'):
                # Only enter folders that are likely to have your 2017 data
                # We skip the main '2017/' folder as requested
                if ('2017' in href or '_txt' in href) and href != '2017/':
                    print(f"üìÇ Entering: {href}")
                    download_txt_recursive(full_url) # Python 'dives' here
                    # When this line finishes, Python 'returns' here automatically!
                else:
                    continue

            # --- FILE LOGIC ---
            elif href.endswith('.txt'):
                if '2017' in href:
                    local_path = os.path.join(SAVE_DIR, href)
                    if not os.path.exists(local_path):
                        print(f"üì• Downloading: {href}")
                        file_res = requests.get(full_url, headers=HEADERS)
                        with open(local_path, 'wb') as f:
                            f.write(file_res.content)
                        time.sleep(0.1)

    except Exception as e:
        print(f"‚ö†Ô∏è Snag at {url}: {e}")

if __name__ == "__main__":
    print("üöÄ Starting Smart-Return Scrubber...")
    download_txt_recursive(BASE_URL)
    print(f"üèÅ Done! Total unique folders visited: {len(visited_urls)}")